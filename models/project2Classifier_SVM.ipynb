{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X are: 1\n",
      "Shape of X is (26216,)\n",
      "Size of X is 26216\n",
      "Dimensions of y are: 2\n",
      "Shape of y is (26216, 35)\n",
      "Size of y is 917560\n",
      "X_train Dim are: 1 Shape= (19662,) Size = 19662\n",
      "y_train Dim are: 2 Shape= (19662, 35) Size = 688170\n",
      "X_test  Dim are: 1 Shape= (6554,) Size = 6554\n",
      "y_test  Dim are: 2 Shape= (6554, 35) Size = 229390\n",
      "dict_keys(['memory', 'steps', 'vect', 'tfidf', 'scaler', 'svc', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'scaler__copy', 'scaler__with_mean', 'scaler__with_std', 'svc__estimator__C', 'svc__estimator__cache_size', 'svc__estimator__class_weight', 'svc__estimator__coef0', 'svc__estimator__decision_function_shape', 'svc__estimator__degree', 'svc__estimator__gamma', 'svc__estimator__kernel', 'svc__estimator__max_iter', 'svc__estimator__probability', 'svc__estimator__random_state', 'svc__estimator__shrinking', 'svc__estimator__tol', 'svc__estimator__verbose', 'svc__estimator', 'svc__n_jobs'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    }
   ],
   "source": [
    "# Create pipline with different model, support vector machine (SVM)\n",
    "\n",
    "# load data from database\n",
    "engine = create_engine(\"sqlite:///DisasterResponse.db\")\n",
    "connection = engine.connect()\n",
    "query = \"SELECT * FROM DisasterResponse\"  ## limit 1000 WHERE related <> 2\n",
    "df = pd.read_sql(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Keep only the predictors in the X\n",
    "predictors = [\"message\"]\n",
    "X = df[predictors].message.values\n",
    "print(\"Dimensions of X are:\", X.ndim)\n",
    "print(\"Shape of X is\", X.shape)\n",
    "print(\"Size of X is\", X.size)\n",
    "\n",
    "# keep ony the 35 response variables in y; dropped child_alone since all values are 0\n",
    "y = df.loc[:, ~df.columns.\n",
    "           isin(['id', 'message', 'original', 'genre', 'child_alone'])]\n",
    "y.head()\n",
    "print(\"Dimensions of y are:\", y.ndim)\n",
    "print(\"Shape of y is\", y.shape)\n",
    "print(\"Size of y is\", y.size)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # Normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words and lemmatize\n",
    "    tokens = [w for w in tokens if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "# let's split the data as train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    train_size=0.75,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# let's check the train test split results\n",
    "print(\"X_train Dim are:\", X_train.ndim, \"Shape=\", X_train.shape, \"Size =\",\n",
    "      X_train.size)\n",
    "print(\"y_train Dim are:\", y_train.ndim, \"Shape=\", y_train.shape, \"Size =\",\n",
    "      y_train.size)\n",
    "print(\"X_test  Dim are:\", X_test.ndim, \"Shape=\", X_test.shape, \"Size =\",\n",
    "      X_test.size)\n",
    "print(\"y_test  Dim are:\", y_test.ndim, \"Shape=\", y_test.shape, \"Size =\",\n",
    "      y_test.size)\n",
    "\n",
    "# Define pipeline for SVC\n",
    "pipeline = Pipeline([('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('scaler', StandardScaler()),\n",
    "                     ('svc', MultiOutputClassifier(SVC(gamma='auto')))])\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "print(pipeline.get_params().keys())\n",
    "\n",
    "# parms for pipline with different model\n",
    "parameters = {\n",
    "    'scaler__with_mean': [False],\n",
    "    'svc__estimator__C': [1.0],  # Regularization parameter\n",
    "#    'svc__n_jobs': [2],  # Number of CPU cores used when parallelizing over classes\n",
    "    'svc__estimator__verbose': [1]\n",
    "}\n",
    "# define GridSearchCV\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "# Fit the pipline with different model\n",
    "import time\n",
    "\n",
    "starttm = time.time()\n",
    "cv.fit(X_train, y_train)\n",
    "endtm = time.time()\n",
    "execTmsec = (endtm - starttm) * 10**6\n",
    "print(\"execution time for the fit=\", execTmsec, \"seconds\")\n",
    "\n",
    "# what are the best parms\n",
    "cv.best_params_\n",
    "\n",
    "# what are the overall results from the new model\n",
    "cv.cv_results_\n",
    "\n",
    "# Use the pipeline with GridSearch to make predictions on test data\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "# score on training dataset\n",
    "print(\"\\n score on training dataset:\", cv.score(X_train, y_train))\n",
    "\n",
    "# score on test dataset\n",
    "print(\"\\n score on test dataset:\", cv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
